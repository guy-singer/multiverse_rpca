{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "853b665a",
   "metadata": {},
   "source": [
    "# Robust PCA (RPCA) for Enigma's Multiverse Model\n",
    "\n",
    "---\n",
    "\n",
    "## 1  RPCA\n",
    "\n",
    "### 1.1  From Ordinary PCA to Robust Decomposition\n",
    "* **Standard PCA** factorises a centred data matrix  \n",
    "  $D\\in\\mathbb{R}^{m\\times n}$ as $D = U\\Sigma V^{\\!\\top}$ and retains the top-$r$ singular vectors.  \n",
    "* One gross outlier can rotate every principal component because the least-squares objective  \n",
    "  $\\displaystyle \\min_{\\operatorname{rank}(L)\\le r}\\lVert D-L\\rVert_F^2$  \n",
    "  assumes small, Gaussian noise.\n",
    "\n",
    "### 1.2  The RPCA Model  \n",
    "Robust PCA assumes the **additive, two-part** model  \n",
    "\n",
    "$\n",
    "\\boxed{D \\;=\\; L \\;+\\; S},\n",
    "$\n",
    "\n",
    "| Symbol | Meaning | Constraint |\n",
    "|--------|---------|------------|\n",
    "| $L$    | Global, coherent structure | **Low rank** |\n",
    "| $S$    | Rare, possibly large deviations | **Sparse** (most entries $=0$) |\n",
    "\n",
    "### 1.3  Principal-Component Pursuit (PCP)\n",
    "\n",
    "RPCA is cast as the convex programme  \n",
    "\n",
    "$\n",
    "\\min_{L,S}\\; \\lVert L\\rVert_* \\;+\\; \\lambda\\,\\lVert S\\rVert_1\n",
    "\\quad \\text{s.t.} \\quad\n",
    "D \\;=\\; L \\;+\\; S,\n",
    "$\n",
    "\n",
    "where  \n",
    "\n",
    "* $\\lVert L\\rVert_* \\;=\\; \\sum_i \\sigma_i(L)$ (the **nuclear norm**) promotes low rank,  \n",
    "* $\\lVert S\\rVert_1$ promotes sparsity,  \n",
    "* $\\lambda \\approx 1/\\sqrt{\\max(m,n)}$ balances the two terms.\n",
    "\n",
    "**Exact-recovery theorem** (Candès et al., 2011): if  \n",
    "\n",
    "1. $L$ is *incoherent* with the coordinate axes, and  \n",
    "2. $S$ has at most $\\rho mn$ non-zeros (small $\\rho$),\n",
    "\n",
    "then PCP returns the true $L$ and $S$.\n",
    "\n",
    "### 1.4  Algorithm Families (Sketch)\n",
    "\n",
    "| Family | Core idea | Per-iteration cost |\n",
    "|--------|-----------|--------------------|\n",
    "| Augmented-Lagrange / Inexact ALM | Alternate **soft-threshold $S$** and **SV-shrink $L$** | $O(mn\\min\\{m,n\\})$ |\n",
    "| ADMM | Split (1), parallelise updates | Similar to ALM |\n",
    "| Non-convex factorisation | Write $L = AB^{\\!\\top}$ with $r\\ll\\min(m,n)$; SGD on $(A,B)$ | $O((m+n)r^2)$ |\n",
    "| Streaming (GRASTA, incPCP) | Incrementally update a low-dim subspace | $O(dr)$ per sample |\n",
    "\n",
    "---\n",
    "\n",
    "## 2  Why RPCA Helps Enigma’s **Multiverse** World Model\n",
    "\n",
    "| Multiverse pain-point | RPCA viewpoint | Mitigation |\n",
    "|-----------------------|----------------|------------|\n",
    "| **Cross-view consistency** (two cameras should see the same crash) | Stack the two $3$-channel images → $6$-channel column; shared 3-D scene is **low rank**, per-camera parallax/occlusion is **sparse** | Decompose each frame stack, train diffusion U-Net on $L$ (shared), per-view heads refine $S$ |\n",
    "| **Long-range context eats VRAM** (tripled context) | Across time, track + cars span tiny latent subspace ⇒ **rank $\\ll HW$** | Run spatio-temporal RPCA on a *(pixels × time)* matrix; store only low-rank coefficients + sparse event tensor → longer horizons per GB |\n",
    "| **Dataset mis-sync noise** (two replays stitched by CV) | Mis-aligned frames appear as **column-sparse** outliers | Streaming RPCA flags columns with large $S$; drop or down-weight before training |\n",
    "\n",
    "---\n",
    "\n",
    "## 3  General Roles of RPCA in World Models\n",
    "\n",
    "| Pipeline stage | RPCA action | Benefit |\n",
    "|----------------|-------------|---------|\n",
    "| **Frame pre-processing** | $D$ = vectorised frames; feed $L$ to model, keep $S$ as event mask | Removes flicker & jitter; isolates transients |\n",
    "| **Latent-trajectory cleaning** | Stack latent states $z_t$; apply RPCA | Reduces model bias; stabilises imagination roll-outs |\n",
    "| **Dataset curation** | RPCA on replay buffer; outlier episodes $\\leftrightarrow$ high magnitude in $S$ | Higher sample efficiency, fewer crashes |\n",
    "| **Error diagnosis** | RPCA on residuals $(o_t-\\hat o_t)$ | Low-rank systematic errors vs. sparse rare failures |\n",
    "| **Multi-agent** | Stack agent streams row-wise | Low-rank = shared physics; sparse = agent-specific shocks |\n",
    "\n",
    "---\n",
    "\n",
    "## 4  Integration Recipes\n",
    "\n",
    "### 4.1  Pre-Processing Approach\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from rpca import R_pca\n",
    "\n",
    "# Example: H×W×6 frames for two cameras, T timesteps\n",
    "D = frames.reshape(H*W*6, T)          # (pixels×channels) × time\n",
    "L, S = R_pca(D, lmbda=1/np.sqrt(max(D.shape))).fit()\n",
    "\n",
    "clean_frames = L.reshape(H, W, 6, T)  # input to world model\n",
    "sparse_mask  = (S != 0)               # optional conditioning\n",
    "\n",
    "Matrix layout choices\n",
    "\t•\t• Pixels × time → long-horizon compression\n",
    "\t•\t• Pixels × channels → cross-view coupling\n",
    "```\n",
    "\n",
    "⸻\n",
    "\n",
    "### 4.2  Loss-Engineering Approach\n",
    "\n",
    "$\n",
    "\\mathcal{L} =\n",
    "\\lambda_{\\text{shared}} \\bigl\\lVert \\hat L - L \\bigr\\rVert_2^2\n",
    ";+;\n",
    "\\lambda_{\\text{sparse}} \\lVert \\hat S - S \\rVert_1\n",
    ";+;\n",
    "\\mathcal{L}_{\\text{diffusion / RL}}.\n",
    "$\n",
    "\n",
    "- Train backbone on $L$ (global scene).\n",
    "- Lightweight heads or conditional tokens reconstruct $S$ (events).\n",
    "- Anneal $\\lambda_{\\text{sparse}}$ after global structure is learnt.\n",
    "\n",
    "⸻\n",
    "\n",
    "### 4.3  Optional Nuclear-Norm Regulariser inside U-Net\n",
    "\n",
    "Add to an intermediate feature map $F$:\n",
    "\n",
    "$\n",
    "\\mathcal{L}{\\text{rank}} ;=; \\beta,\\lVert F\\rVert*,\n",
    "$\n",
    "\n",
    "nudging hidden activations toward low rank — a differentiable analogue of RPCA.\n",
    "\n",
    "⸻\n",
    "\n",
    "\n",
    "\n",
    "### References\n",
    "\t•\tCandès E. J., Li X., Ma Y., Wright J. — “Robust Principal Component Analysis?” JACM 58 (3), 2011\n",
    "\t•\tEnigma Labs blog — “Introducing Multiverse: The First AI Multiplayer World Model” (May 2025)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233b3684",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
